{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP5lNjA1a9p6xSe0odWZW4k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubaHesham11/machine-translation/blob/main/machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SamirMoustafa/nmt-with-attention-for-ar-to-en.git"
      ],
      "metadata": {
        "id": "yE8fIFL9qhmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "with open('/content/nmt-with-attention-for-ar-to-en/ara_.txt', 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Split data into source and target sentences\n",
        "source_sentences = []\n",
        "target_sentences = []\n",
        "for line in lines:\n",
        "    parts = line.strip().split('\\t')\n",
        "    source_sentences.append(parts[0])\n",
        "    target_sentences.append(parts[1])\n",
        "\n",
        "# Tokenize the sentences and convert them to sequences of integers\n",
        "source_tokenizer = Tokenizer()\n",
        "source_tokenizer.fit_on_texts(source_sentences)\n",
        "\n",
        "# Modify the tokenization for the target language to include '<start>' token\n",
        "target_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "target_sentences_with_start = ['<start> ' + text for text in target_sentences]  # Add '<start>' to the beginning of each sentence\n",
        "target_tokenizer.fit_on_texts(target_sentences_with_start)\n",
        "\n",
        "# Ensure '<start>' is added to the vocabulary and padding index is reserved\n",
        "target_tokenizer.word_index['<start>'] = len(target_tokenizer.word_index) + 1\n",
        "target_tokenizer.index_word[len(target_tokenizer.word_index)] = '<start>'\n",
        "target_tokenizer.word_index['<pad>'] = 0\n",
        "\n",
        "source_sequences = source_tokenizer.texts_to_sequences(source_sentences)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(target_sentences_with_start)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_source_length = max(len(seq) for seq in source_sequences)\n",
        "max_target_length = max(len(seq) for seq in target_sequences)\n",
        "padded_source_sequences = pad_sequences(source_sequences, maxlen=max_source_length, padding='post')\n",
        "padded_target_sequences = pad_sequences(target_sequences, maxlen=max_target_length, padding='post')\n",
        "\n",
        "# Create numpy arrays for training data\n",
        "encoder_input_data = np.array(padded_source_sequences)\n",
        "decoder_input_data = np.array(padded_target_sequences[:, :-1])  # Remove the last token\n",
        "decoder_target_data = np.array(padded_target_sequences[:, 1:])   # Remove the first token\n"
      ],
      "metadata": {
        "id": "jwbs9xb-qi_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test sets\n",
        "encoder_input_train, encoder_input_test, decoder_input_train, decoder_input_test, decoder_target_train, decoder_target_test = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the train and test sets\n",
        "print(\"Shape of encoder input train set:\", encoder_input_train.shape)\n",
        "print(\"Shape of decoder input train set:\", decoder_input_train.shape)\n",
        "print(\"Shape of decoder target train set:\", decoder_target_train.shape)\n",
        "print(\"Shape of encoder input test set:\", encoder_input_test.shape)\n",
        "print(\"Shape of decoder input test set:\", decoder_input_test.shape)\n",
        "print(\"Shape of decoder target test set:\", decoder_target_test.shape)\n"
      ],
      "metadata": {
        "id": "9AaiogdNqi8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trying\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Attention, Dropout\n",
        "\n",
        "# Define model hyperparameters\n",
        "embedding_dim = 256\n",
        "hidden_units = 1024\n",
        "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "dropout_rate = 0.2  # Adjust as needed\n",
        "\n",
        "# Define encoder input\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(source_vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = Bidirectional(LSTM(hidden_units, return_sequences=True, return_state=True))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "# Define decoder input\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(hidden_units*2, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention mechanism\n",
        "attention = Attention()\n",
        "context_vector = attention([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Concatenate context vector and decoder output\n",
        "decoder_combined_context = Concatenate(axis=-1)([decoder_outputs, context_vector])\n",
        "\n",
        "# Apply dropout for regularization\n",
        "decoder_dropout = Dropout(dropout_rate)\n",
        "decoder_outputs = decoder_dropout(decoder_combined_context)\n",
        "\n",
        "# Output layer\n",
        "decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "a9eqUyfNqi1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit([encoder_input_train, decoder_input_train], decoder_target_train, batch_size=64, epochs=50, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06AAjIHkqivM",
        "outputId": "996354c2-e2a0-4267-e235-da481c0ba898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "108/108 [==============================] - 51s 393ms/step - loss: 1.2940 - accuracy: 0.8703 - val_loss: 1.0497 - val_accuracy: 0.8792\n",
            "Epoch 2/50\n",
            "108/108 [==============================] - 41s 383ms/step - loss: 0.9644 - accuracy: 0.8813 - val_loss: 1.0293 - val_accuracy: 0.8803\n",
            "Epoch 3/50\n",
            "108/108 [==============================] - 42s 392ms/step - loss: 0.9111 - accuracy: 0.8828 - val_loss: 1.0166 - val_accuracy: 0.8823\n",
            "Epoch 4/50\n",
            "108/108 [==============================] - 43s 395ms/step - loss: 0.8611 - accuracy: 0.8849 - val_loss: 1.0124 - val_accuracy: 0.8837\n",
            "Epoch 5/50\n",
            "108/108 [==============================] - 42s 391ms/step - loss: 0.8067 - accuracy: 0.8871 - val_loss: 1.0058 - val_accuracy: 0.8858\n",
            "Epoch 6/50\n",
            "108/108 [==============================] - 41s 379ms/step - loss: 0.7439 - accuracy: 0.8899 - val_loss: 1.0058 - val_accuracy: 0.8876\n",
            "Epoch 7/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.6688 - accuracy: 0.8932 - val_loss: 1.0116 - val_accuracy: 0.8892\n",
            "Epoch 8/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.5836 - accuracy: 0.8973 - val_loss: 1.0179 - val_accuracy: 0.8901\n",
            "Epoch 9/50\n",
            "108/108 [==============================] - 42s 394ms/step - loss: 0.4920 - accuracy: 0.9043 - val_loss: 1.0418 - val_accuracy: 0.8909\n",
            "Epoch 10/50\n",
            "108/108 [==============================] - 42s 394ms/step - loss: 0.4020 - accuracy: 0.9151 - val_loss: 1.0649 - val_accuracy: 0.8916\n",
            "Epoch 11/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.3148 - accuracy: 0.9290 - val_loss: 1.0952 - val_accuracy: 0.8931\n",
            "Epoch 12/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.2410 - accuracy: 0.9433 - val_loss: 1.1110 - val_accuracy: 0.8926\n",
            "Epoch 13/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.1813 - accuracy: 0.9566 - val_loss: 1.1381 - val_accuracy: 0.8935\n",
            "Epoch 14/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.1362 - accuracy: 0.9672 - val_loss: 1.1608 - val_accuracy: 0.8954\n",
            "Epoch 15/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.1020 - accuracy: 0.9757 - val_loss: 1.1815 - val_accuracy: 0.8971\n",
            "Epoch 16/50\n",
            "108/108 [==============================] - 43s 394ms/step - loss: 0.0773 - accuracy: 0.9819 - val_loss: 1.1954 - val_accuracy: 0.8970\n",
            "Epoch 17/50\n",
            "108/108 [==============================] - 41s 378ms/step - loss: 0.0599 - accuracy: 0.9864 - val_loss: 1.2023 - val_accuracy: 0.8982\n",
            "Epoch 18/50\n",
            "108/108 [==============================] - 43s 395ms/step - loss: 0.0496 - accuracy: 0.9890 - val_loss: 1.2193 - val_accuracy: 0.8979\n",
            "Epoch 19/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.0426 - accuracy: 0.9906 - val_loss: 1.2212 - val_accuracy: 0.8981\n",
            "Epoch 20/50\n",
            "108/108 [==============================] - 42s 394ms/step - loss: 0.0358 - accuracy: 0.9922 - val_loss: 1.2358 - val_accuracy: 0.8982\n",
            "Epoch 21/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.0316 - accuracy: 0.9933 - val_loss: 1.2403 - val_accuracy: 0.8985\n",
            "Epoch 22/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.0293 - accuracy: 0.9939 - val_loss: 1.2441 - val_accuracy: 0.8992\n",
            "Epoch 23/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.0263 - accuracy: 0.9946 - val_loss: 1.2527 - val_accuracy: 0.8998\n",
            "Epoch 24/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.0234 - accuracy: 0.9952 - val_loss: 1.2516 - val_accuracy: 0.8992\n",
            "Epoch 25/50\n",
            "108/108 [==============================] - 41s 378ms/step - loss: 0.0215 - accuracy: 0.9955 - val_loss: 1.2545 - val_accuracy: 0.9002\n",
            "Epoch 26/50\n",
            "108/108 [==============================] - 41s 376ms/step - loss: 0.0216 - accuracy: 0.9956 - val_loss: 1.2549 - val_accuracy: 0.8993\n",
            "Epoch 27/50\n",
            "108/108 [==============================] - 42s 391ms/step - loss: 0.0210 - accuracy: 0.9956 - val_loss: 1.2643 - val_accuracy: 0.8989\n",
            "Epoch 28/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.0202 - accuracy: 0.9956 - val_loss: 1.2602 - val_accuracy: 0.8994\n",
            "Epoch 29/50\n",
            "108/108 [==============================] - 41s 376ms/step - loss: 0.0193 - accuracy: 0.9959 - val_loss: 1.2629 - val_accuracy: 0.8996\n",
            "Epoch 30/50\n",
            "108/108 [==============================] - 42s 394ms/step - loss: 0.0187 - accuracy: 0.9959 - val_loss: 1.2559 - val_accuracy: 0.8981\n",
            "Epoch 31/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.0182 - accuracy: 0.9960 - val_loss: 1.2665 - val_accuracy: 0.8993\n",
            "Epoch 32/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.0181 - accuracy: 0.9960 - val_loss: 1.2668 - val_accuracy: 0.8998\n",
            "Epoch 33/50\n",
            "108/108 [==============================] - 41s 376ms/step - loss: 0.0179 - accuracy: 0.9960 - val_loss: 1.2641 - val_accuracy: 0.9000\n",
            "Epoch 34/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.0187 - accuracy: 0.9959 - val_loss: 1.2672 - val_accuracy: 0.8993\n",
            "Epoch 35/50\n",
            "108/108 [==============================] - 42s 392ms/step - loss: 0.0194 - accuracy: 0.9957 - val_loss: 1.2563 - val_accuracy: 0.8989\n",
            "Epoch 36/50\n",
            "108/108 [==============================] - 41s 378ms/step - loss: 0.0197 - accuracy: 0.9956 - val_loss: 1.2589 - val_accuracy: 0.8976\n",
            "Epoch 37/50\n",
            "108/108 [==============================] - 42s 394ms/step - loss: 0.0212 - accuracy: 0.9953 - val_loss: 1.2578 - val_accuracy: 0.8985\n",
            "Epoch 38/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.0224 - accuracy: 0.9950 - val_loss: 1.2557 - val_accuracy: 0.8982\n",
            "Epoch 39/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.0248 - accuracy: 0.9945 - val_loss: 1.2492 - val_accuracy: 0.8985\n",
            "Epoch 40/50\n",
            "108/108 [==============================] - 42s 392ms/step - loss: 0.0259 - accuracy: 0.9940 - val_loss: 1.2564 - val_accuracy: 0.8983\n",
            "Epoch 41/50\n",
            "108/108 [==============================] - 42s 394ms/step - loss: 0.0248 - accuracy: 0.9943 - val_loss: 1.2626 - val_accuracy: 0.8985\n",
            "Epoch 42/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.0224 - accuracy: 0.9949 - val_loss: 1.2658 - val_accuracy: 0.8985\n",
            "Epoch 43/50\n",
            "108/108 [==============================] - 43s 394ms/step - loss: 0.0206 - accuracy: 0.9953 - val_loss: 1.2616 - val_accuracy: 0.8986\n",
            "Epoch 44/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.0181 - accuracy: 0.9958 - val_loss: 1.2579 - val_accuracy: 0.8984\n",
            "Epoch 45/50\n",
            "108/108 [==============================] - 42s 394ms/step - loss: 0.0163 - accuracy: 0.9961 - val_loss: 1.2582 - val_accuracy: 0.8999\n",
            "Epoch 46/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.0160 - accuracy: 0.9961 - val_loss: 1.2591 - val_accuracy: 0.8990\n",
            "Epoch 47/50\n",
            "108/108 [==============================] - 42s 393ms/step - loss: 0.0149 - accuracy: 0.9962 - val_loss: 1.2538 - val_accuracy: 0.8998\n",
            "Epoch 48/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.0149 - accuracy: 0.9962 - val_loss: 1.2511 - val_accuracy: 0.9000\n",
            "Epoch 49/50\n",
            "108/108 [==============================] - 41s 377ms/step - loss: 0.0139 - accuracy: 0.9963 - val_loss: 1.2615 - val_accuracy: 0.9003\n",
            "Epoch 50/50\n",
            "108/108 [==============================] - 42s 392ms/step - loss: 0.0141 - accuracy: 0.9963 - val_loss: 1.2447 - val_accuracy: 0.9001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate([encoder_input_test, decoder_input_test], decoder_target_test)\n",
        "\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlj-hgp7qisQ",
        "outputId": "cae4f9e6-f239-4c15-d1c1-35207e603c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 6s 81ms/step - loss: 1.2146 - accuracy: 0.9016\n",
            "Test Loss: 1.214596152305603\n",
            "Test Accuracy: 0.9015691876411438\n"
          ]
        }
      ]
    }
  ]
}